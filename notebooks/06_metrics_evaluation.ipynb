{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89b6e860-722f-4997-85a9-15a753aafb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Using model: stackridge (test_preds_ensemble.csv)\n",
      "âš ï¸ No OOF predictions found for 'stackridge'. Skipping OOF evaluation.\n",
      "âœ… Loaded: c:\\Users\\dylan\\Documents\\Projects\\horse_model_project\\outputs\\test_preds_ensemble.csv with 11275 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dylan\\AppData\\Local\\Temp\\ipykernel_27588\\2729564772.py:109: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  race_margin = df.groupby('Race_ID').apply(top2_margin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Exported race-level edge metrics\n",
      "âœ… Exported top safest picks\n",
      "âœ… Exported top longshots\n",
      "âœ… Exported confidence bin summary\n",
      "ğŸ¯ High-confidence horses (>85%): 0\n",
      "ğŸ§ Low-confidence horses (<2%): 0\n",
      "âœ… Exported test_predictions_with_edge.csv including Blended_Probability\n",
      "âœ… Exported top mispriced horses\n",
      "\n",
      "âœ… Evaluation & summary complete. Ready for 07_visuals or 08_edge_analysis.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ğŸ“Ÿ 06_metrics_evaluation.ipynb â€“ Evaluation & Summary\n",
    "# -----------------------------------------------------\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import log_loss, brier_score_loss\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# === Path Setup ===\n",
    "BASE = Path.cwd()\n",
    "PROJECT_ROOT = BASE if (BASE / \"src\").exists() else BASE.parent\n",
    "sys.path.append(str(PROJECT_ROOT / \"src\"))\n",
    "OUT_DIR = PROJECT_ROOT / \"outputs\"\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "MARKET_PATH = PROJECT_ROOT / \"data\" / \"market\" / \"betfair_sp.csv\"\n",
    "\n",
    "# === CONFIG: Load Model Selection ===\n",
    "CONFIG_PATH = PROJECT_ROOT / \"config.yaml\"\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "default_model = config[\"default_model\"]\n",
    "prediction_files = config[\"prediction_files\"]\n",
    "column_map = config.get(\"column_map\", {})\n",
    "\n",
    "PRED_FILE = prediction_files[default_model]\n",
    "PRED_PATH = OUT_DIR / PRED_FILE\n",
    "\n",
    "# Decide OOF file for baseline/ensemble models\n",
    "if default_model in [\"lgbm\", \"LGBM\"]:\n",
    "    OOF_FILE = \"oof_preds_lgbm.csv\"\n",
    "elif default_model in [\"ensemble\", \"stackridge\"]:\n",
    "    OOF_FILE = f\"oof_preds_{default_model}.csv\"\n",
    "else:\n",
    "    OOF_FILE = f\"oof_preds_{default_model}.csv\"\n",
    "OOF_PATH = OUT_DIR / OOF_FILE\n",
    "\n",
    "print(f\"ğŸ” Using model: {default_model} ({PRED_FILE})\")\n",
    "\n",
    "# === Pick column for prediction based on config ===\n",
    "pred_col = column_map.get(default_model, \"Predicted_Probability\")\n",
    "\n",
    "# %%\n",
    "# === 1. Load OOF Predictions & Evaluate ===\n",
    "try:\n",
    "    oof_df = pd.read_csv(OOF_PATH)\n",
    "    assert {'True_Label', 'Predicted_Probability', 'Race_ID'}.issubset(oof_df.columns)\n",
    "\n",
    "    target = oof_df[\"True_Label\"]\n",
    "    oof_preds = oof_df[\"Predicted_Probability\"]\n",
    "    race_ids = oof_df[\"Race_ID\"]\n",
    "\n",
    "    print(f\"âœ… Loaded OOF predictions: {len(oof_df)} samples\")\n",
    "\n",
    "    # --- Calibration Curve ---\n",
    "    prob_true, prob_pred = calibration_curve(target, oof_preds, n_bins=10, strategy='uniform')\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(prob_pred, prob_true, marker='o', label='Model Calibration')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect Calibration')\n",
    "    plt.title(\"Reliability Curve â€“ Model Only\")\n",
    "    plt.xlabel(\"Predicted Probability\")\n",
    "    plt.ylabel(\"Empirical Win Rate\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Log Loss & Brier Score ---\n",
    "    log = log_loss(target, oof_preds)\n",
    "    brier = brier_score_loss(target, oof_preds)\n",
    "    print(f\"ğŸ“‰ Log Loss: {log:.5f}\")\n",
    "    print(f\"ğŸŒ§ï¸  Brier Score: {brier:.5f}\")\n",
    "\n",
    "    # --- Confidence Bin Summary ---\n",
    "    oof_df[\"Prob_Bin\"] = pd.cut(oof_preds, bins=np.linspace(0, 1, 11))\n",
    "    summary = oof_df.groupby(\"Prob_Bin\")[\"True_Label\"].agg(['count', 'mean']).reset_index()\n",
    "    summary.columns = [\"Bin\", \"Samples\", \"Empirical Win Rate\"]\n",
    "    print(summary)\n",
    "\n",
    "    # --- Save ---\n",
    "    eval_df = pd.DataFrame({\n",
    "        \"Log_Loss\": [log],\n",
    "        \"Brier_Score\": [brier]\n",
    "    })\n",
    "    eval_df.to_csv(OUT_DIR / \"eval_metrics.csv\", index=False)\n",
    "    print(\"âœ… Saved evaluation metrics\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âš ï¸ No OOF predictions found for '{default_model}'. Skipping OOF evaluation.\")\n",
    "\n",
    "\n",
    "# %%\n",
    "# === 2. Load Test Predictions & Race-Level Metrics ===\n",
    "df = pd.read_csv(PRED_PATH)\n",
    "assert {'Race_ID', 'Horse', pred_col}.issubset(df.columns)\n",
    "print(f\"âœ… Loaded: {PRED_PATH} with {len(df)} rows\")\n",
    "\n",
    "# Race-level metrics\n",
    "def top2_margin(group):\n",
    "    sorted_probs = group[pred_col].sort_values(ascending=False)\n",
    "    return sorted_probs.iloc[0] - sorted_probs.iloc[1]\n",
    "\n",
    "race_margin = df.groupby('Race_ID').apply(top2_margin)\n",
    "race_entropy = df.groupby('Race_ID')[pred_col].apply(entropy)\n",
    "field_size = df.groupby('Race_ID').size()\n",
    "adj_confidence = df.groupby('Race_ID')[pred_col].max() / (1 / field_size)\n",
    "\n",
    "race_edge = pd.DataFrame({\n",
    "    'Race_ID': race_margin.index,\n",
    "    'Margin_Top2': race_margin.values,\n",
    "    'Entropy': race_entropy.values,\n",
    "    'Field_Size': field_size.values,\n",
    "    'Adj_Confidence': adj_confidence.values\n",
    "})\n",
    "race_edge.to_csv(OUT_DIR / \"summary_race_edge.csv\", index=False)\n",
    "print(\"âœ… Exported race-level edge metrics\")\n",
    "\n",
    "# %%\n",
    "# === 3. Top Picks, Longshots, Confidence Bins ===\n",
    "df_sorted = df.sort_values(pred_col, ascending=False)\n",
    "top_preds = df_sorted.groupby(\"Race_ID\").first().reset_index()\n",
    "top_safe = top_preds.sort_values(pred_col, ascending=False).head(10)\n",
    "top_safe.to_csv(OUT_DIR / \"summary_top_safe.csv\", index=False)\n",
    "print(\"âœ… Exported top safest picks\")\n",
    "\n",
    "df[\"Rank_in_Race\"] = df.groupby(\"Race_ID\")[pred_col].rank(ascending=False, method='first')\n",
    "longshots = df[(df[pred_col] >= 0.10) & (df[\"Rank_in_Race\"] > 1)]\n",
    "longshots = longshots.sort_values(pred_col, ascending=False).head(10)\n",
    "longshots.to_csv(OUT_DIR / \"summary_longshots.csv\", index=False)\n",
    "print(\"âœ… Exported top longshots\")\n",
    "\n",
    "# Confidence bins\n",
    "conf_bins = pd.cut(df[pred_col], bins=np.linspace(0, 1, 11))\n",
    "conf_counts = conf_bins.value_counts().sort_index()\n",
    "conf_summary = conf_counts.reset_index()\n",
    "conf_summary.columns = [\"Bin\", \"Count\"]\n",
    "conf_summary.to_csv(OUT_DIR / \"summary_confidence_bins.csv\", index=False)\n",
    "print(\"âœ… Exported confidence bin summary\")\n",
    "\n",
    "print(f\"ğŸ¯ High-confidence horses (>85%): {len(df[df[pred_col] > 0.85])}\")\n",
    "print(f\"ğŸ§ Low-confidence horses (<2%): {len(df[df[pred_col] < 0.02])}\")\n",
    "\n",
    "# %%\n",
    "# === 4. Edge Score vs Market + Blended Probability ===\n",
    "market_df = pd.read_csv(MARKET_PATH)\n",
    "df_edge = df.merge(market_df, on=[\"Race_ID\", \"Horse\"], how=\"left\")\n",
    "\n",
    "df_edge[\"Market_Prob\"] = 1 / df_edge[\"Market_Odds\"]\n",
    "df_edge[\"Edge_Score\"] = df_edge[pred_col] - df_edge[\"Market_Prob\"]\n",
    "\n",
    "# --- Compute Blended Probability ---\n",
    "ALPHA = 0.7  # Static blending weight\n",
    "df_edge[\"Blended_Probability\"] = ALPHA * df_edge[pred_col] + (1 - ALPHA) * df_edge[\"Market_Prob\"]\n",
    "\n",
    "# Save blended predictions\n",
    "df_edge.to_csv(OUT_DIR / \"test_predictions_with_edge.csv\", index=False)\n",
    "print(\"âœ… Exported test_predictions_with_edge.csv including Blended_Probability\")\n",
    "\n",
    "# Top mispriced\n",
    "mispriced_top = df_edge.sort_values(\"Edge_Score\", ascending=False).head(10)\n",
    "mispriced_top.to_csv(OUT_DIR / \"summary_top_mispriced.csv\", index=False)\n",
    "print(\"âœ… Exported top mispriced horses\")\n",
    "\n",
    "# %%\n",
    "# === 5. Evaluate Blended Probability (Optional Section) ===\n",
    "if 'True_Label' in df_edge.columns:\n",
    "    y_true = df_edge[\"True_Label\"]\n",
    "    y_blend = df_edge[\"Blended_Probability\"]\n",
    "\n",
    "    log_blend = log_loss(y_true, y_blend)\n",
    "    brier_blend = brier_score_loss(y_true, y_blend)\n",
    "\n",
    "    # Calibration curve\n",
    "    prob_true_blend, prob_pred_blend = calibration_curve(y_true, y_blend, n_bins=10)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(prob_pred_blend, prob_true_blend, marker='o', label='Blended')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect')\n",
    "    plt.title(\"Reliability Curve â€“ Blended Probability\")\n",
    "    plt.xlabel(\"Predicted Probability\")\n",
    "    plt.ylabel(\"Empirical Win Rate\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nğŸ”€ Blended Log Loss: {log_blend:.5f}\")\n",
    "    print(f\"ğŸ”€ Blended Brier Score: {brier_blend:.5f}\")\n",
    "\n",
    "    pd.DataFrame({\n",
    "        \"Log_Loss\": [log_blend],\n",
    "        \"Brier_Score\": [brier_blend],\n",
    "        \"Alpha\": [ALPHA]\n",
    "    }).to_csv(OUT_DIR / \"eval_blended_metrics.csv\", index=False)\n",
    "    print(\"âœ… Saved eval_blended_metrics.csv\")\n",
    "\n",
    "# %%\n",
    "print(\"\\nâœ… Evaluation & summary complete. Ready for 07_visuals or 08_edge_analysis.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f89f185-a38a-425b-bf6b-8334e75f4c26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (horse_model)",
   "language": "python",
   "name": "horse_model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
