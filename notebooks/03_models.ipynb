{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1ff6d4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T19:03:26.080657Z",
     "iopub.status.busy": "2025-06-12T19:03:26.080657Z",
     "iopub.status.idle": "2025-06-12T19:06:47.088533Z",
     "shell.execute_reply": "2025-06-12T19:06:47.088533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Training Fold 1/5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5531\n",
      "[LightGBM] [Info] Number of data points in the train set: 41678, number of used features: 32\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's ndcg@1: 0.404494\tvalid_0's ndcg@2: 0.464407\tvalid_0's ndcg@3: 0.522771\tvalid_0's ndcg@4: 0.563287\tvalid_0's ndcg@5: 0.598259\n",
      "Early stopping, best iteration is:\n",
      "[31]\tvalid_0's ndcg@1: 0.424157\tvalid_0's ndcg@2: 0.470995\tvalid_0's ndcg@3: 0.521938\tvalid_0's ndcg@4: 0.567361\tvalid_0's ndcg@5: 0.60086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Fold 1 ‚Äî Log Loss: 0.31529 | Brier: 0.08914\n",
      "\n",
      "üöÄ Training Fold 2/5...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002609 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5471\n",
      "[LightGBM] [Info] Number of data points in the train set: 41680, number of used features: 32\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's ndcg@1: 0.420806\tvalid_0's ndcg@2: 0.482626\tvalid_0's ndcg@3: 0.530518\tvalid_0's ndcg@4: 0.573032\tvalid_0's ndcg@5: 0.602722\n",
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's ndcg@1: 0.417994\tvalid_0's ndcg@2: 0.476267\tvalid_0's ndcg@3: 0.524956\tvalid_0's ndcg@4: 0.567228\tvalid_0's ndcg@5: 0.605977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Fold 2 ‚Äî Log Loss: 0.31399 | Brier: 0.08874\n",
      "\n",
      "üöÄ Training Fold 3/5...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002602 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5539\n",
      "[LightGBM] [Info] Number of data points in the train set: 41680, number of used features: 32\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's ndcg@1: 0.424555\tvalid_0's ndcg@2: 0.47634\tvalid_0's ndcg@3: 0.531757\tvalid_0's ndcg@4: 0.571788\tvalid_0's ndcg@5: 0.603109\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's ndcg@1: 0.430178\tvalid_0's ndcg@2: 0.476831\tvalid_0's ndcg@3: 0.53516\tvalid_0's ndcg@4: 0.578537\tvalid_0's ndcg@5: 0.605266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Fold 3 ‚Äî Log Loss: 0.31828 | Brier: 0.08958\n",
      "\n",
      "üöÄ Training Fold 4/5...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002610 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5467\n",
      "[LightGBM] [Info] Number of data points in the train set: 41680, number of used features: 32\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's ndcg@1: 0.428304\tvalid_0's ndcg@2: 0.474365\tvalid_0's ndcg@3: 0.524029\tvalid_0's ndcg@4: 0.56499\tvalid_0's ndcg@5: 0.596539\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's ndcg@1: 0.432052\tvalid_0's ndcg@2: 0.480195\tvalid_0's ndcg@3: 0.531125\tvalid_0's ndcg@4: 0.565505\tvalid_0's ndcg@5: 0.601354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Fold 4 ‚Äî Log Loss: 0.31772 | Brier: 0.08951\n",
      "\n",
      "üöÄ Training Fold 5/5...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005182 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5540\n",
      "[LightGBM] [Info] Number of data points in the train set: 41678, number of used features: 32\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's ndcg@1: 0.42603\tvalid_0's ndcg@2: 0.48398\tvalid_0's ndcg@3: 0.540293\tvalid_0's ndcg@4: 0.581146\tvalid_0's ndcg@5: 0.606618\n",
      "Early stopping, best iteration is:\n",
      "[54]\tvalid_0's ndcg@1: 0.431648\tvalid_0's ndcg@2: 0.488093\tvalid_0's ndcg@3: 0.545575\tvalid_0's ndcg@4: 0.583865\tvalid_0's ndcg@5: 0.613681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Fold 5 ‚Äî Log Loss: 0.31209 | Brier: 0.08850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Final Out-of-Fold Evaluation\n",
      "-------------------------------\n",
      "Mean Log Loss:    0.31547\n",
      "Mean Brier Score: 0.08909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ oof_preds.csv and oof_preds_lgbm.csv saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ test_predictions.csv and test_preds_lgbm.csv saved!\n",
      "\n",
      "üå≤ RF Training Fold 1/5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üå≤ RF Training Fold 2/5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üå≤ RF Training Fold 3/5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üå≤ RF Training Fold 4/5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üå≤ RF Training Fold 5/5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: C:\\Users\\dylan\\Documents\\Projects\\horse_model_project\\outputs\\oof_preds_rf.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: C:\\Users\\dylan\\Documents\\Projects\\horse_model_project\\outputs\\test_preds_rf.csv\n",
      "\n",
      "ü¶æ XGB Training Fold 1/5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü¶æ XGB Training Fold 2/5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü¶æ XGB Training Fold 3/5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü¶æ XGB Training Fold 4/5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü¶æ XGB Training Fold 5/5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: C:\\Users\\dylan\\Documents\\Projects\\horse_model_project\\outputs\\oof_preds_xgb.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: C:\\Users\\dylan\\Documents\\Projects\\horse_model_project\\outputs\\test_preds_xgb.csv\n",
      "\n",
      "üêà CatBoost Training Fold 1/5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6254182\ttest: 0.6257008\tbest: 0.6257008 (0)\ttotal: 233ms\tremaining: 1m 56s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100:\tlearn: 0.2169690\ttest: 0.3138206\tbest: 0.3128708 (78)\ttotal: 8.9s\tremaining: 35.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.3128708412\n",
      "bestIteration = 78\n",
      "\n",
      "Shrink model to first 79 iterations.\n",
      "\n",
      "üêà CatBoost Training Fold 2/5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6254590\ttest: 0.6256469\tbest: 0.6256469 (0)\ttotal: 77ms\tremaining: 38.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100:\tlearn: 0.2179266\ttest: 0.3163662\tbest: 0.3144512 (65)\ttotal: 8.35s\tremaining: 33s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.3144512144\n",
      "bestIteration = 65\n",
      "\n",
      "Shrink model to first 66 iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üêà CatBoost Training Fold 3/5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6255274\ttest: 0.6253623\tbest: 0.6253623 (0)\ttotal: 76.6ms\tremaining: 38.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100:\tlearn: 0.2160727\ttest: 0.3150985\tbest: 0.3150567 (98)\ttotal: 8.78s\tremaining: 34.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.3150566777\n",
      "bestIteration = 98\n",
      "\n",
      "Shrink model to first 99 iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üêà CatBoost Training Fold 4/5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6253991\ttest: 0.6257205\tbest: 0.6257205 (0)\ttotal: 83.5ms\tremaining: 41.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100:\tlearn: 0.2162310\ttest: 0.3147368\tbest: 0.3121565 (76)\ttotal: 9.63s\tremaining: 38.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.3121564518\n",
      "bestIteration = 76\n",
      "\n",
      "Shrink model to first 77 iterations.\n",
      "\n",
      "üêà CatBoost Training Fold 5/5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6256269\ttest: 0.6253493\tbest: 0.6253493 (0)\ttotal: 81.6ms\tremaining: 40.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100:\tlearn: 0.2169536\ttest: 0.3098649\tbest: 0.3098649 (100)\ttotal: 9.8s\tremaining: 38.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.309212181\n",
      "bestIteration = 128\n",
      "\n",
      "Shrink model to first 129 iterations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: C:\\Users\\dylan\\Documents\\Projects\\horse_model_project\\outputs\\oof_preds_cat.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: C:\\Users\\dylan\\Documents\\Projects\\horse_model_project\\outputs\\test_preds_cat.csv\n",
      "\n",
      "üß† MLP Training Fold 1/5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† MLP Training Fold 2/5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† MLP Training Fold 3/5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† MLP Training Fold 4/5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† MLP Training Fold 5/5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: C:\\Users\\dylan\\Documents\\Projects\\horse_model_project\\outputs\\oof_preds_mlp.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: C:\\Users\\dylan\\Documents\\Projects\\horse_model_project\\outputs\\test_preds_mlp.csv\n",
      "\n",
      "üéØ All model OOF/test predictions saved for stacking or analysis.\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "# üìü 03_models.ipynb ‚Äì Baseline & Alternative ML Models\n",
    "# -----------------------------------------------------\n",
    "# Trains LightGBM LambdaRank model + alternative models to estimate win probabilities for each horse.\n",
    "# Outputs OOF/test predictions for each, ready for stacking/blending.\n",
    "\n",
    "# %% \n",
    "# === Imports & path setup ===\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR if (NOTEBOOK_DIR / \"src\").exists() else NOTEBOOK_DIR.parent\n",
    "sys.path.append(str(PROJECT_ROOT / \"src\"))\n",
    "\n",
    "from model import train_and_evaluate  # Your LambdaRank function\n",
    "\n",
    "DATA_INTERIM = PROJECT_ROOT / \"data\" / \"interim\"\n",
    "OUTPUTS = PROJECT_ROOT / \"outputs\"\n",
    "OUTPUTS.mkdir(exist_ok=True)\n",
    "\n",
    "# %% \n",
    "# === Load features ===\n",
    "train = pd.read_csv(DATA_INTERIM / \"train_features.csv\")\n",
    "test = pd.read_csv(DATA_INTERIM / \"test_features.csv\")\n",
    "\n",
    "# === Prepare labels and groupings ===\n",
    "target = train[\"Winner\"]\n",
    "groups = train[\"Race_ID\"]  # Each group = a race\n",
    "\n",
    "X = train.drop(columns=[\"Race_ID\", \"Horse\", \"Winner\"])\n",
    "X_test = test.drop(columns=[\"Race_ID\", \"Horse\"], errors=\"ignore\")\n",
    "\n",
    "# === Encode categorical variables ===\n",
    "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "for col in cat_cols:\n",
    "    train_cats = X[col].astype(\"category\").cat.categories\n",
    "    X[col] = X[col].astype(pd.CategoricalDtype(categories=train_cats)).cat.codes\n",
    "    X_test[col] = X_test[col].astype(pd.CategoricalDtype(categories=train_cats)).cat.codes\n",
    "\n",
    "# --- Utility: Softmax per race\n",
    "def softmax_group(df, group_col, score_col):\n",
    "    return df.groupby(group_col)[score_col].transform(lambda x: np.exp(x - np.max(x)) / np.sum(np.exp(x - np.max(x))))\n",
    "\n",
    "def save_predictions(filename, df, prob_col=\"Predicted_Probability\"):\n",
    "    df.to_csv(OUTPUTS / filename, index=False)\n",
    "    print(f\"‚úÖ Saved: {OUTPUTS / filename}\")\n",
    "\n",
    "# ==========================================================\n",
    "# 1. LightGBM LambdaRank Baseline\n",
    "# ==========================================================\n",
    "models, oof_preds, feature_importance = train_and_evaluate(X, target, groups)\n",
    "\n",
    "# OOF for LGBM\n",
    "oof_preds_df = pd.DataFrame({\n",
    "    \"Race_ID\": train[\"Race_ID\"],\n",
    "    \"Horse\": train[\"Horse\"],\n",
    "    \"True_Label\": target,\n",
    "    \"Predicted_Probability\": oof_preds\n",
    "})\n",
    "oof_preds_df.to_csv(OUTPUTS / \"oof_preds.csv\", index=False)\n",
    "oof_preds_df.to_csv(OUTPUTS / \"oof_preds_lgbm.csv\", index=False)\n",
    "print(\"‚úÖ oof_preds.csv and oof_preds_lgbm.csv saved.\")\n",
    "\n",
    "# Test preds for LGBM\n",
    "best_model = models[-1]\n",
    "test_raw_scores = best_model.predict(X_test)\n",
    "test_out = test.copy()\n",
    "test_out[\"Predicted_Probability\"] = softmax_group(test_out.assign(score=test_raw_scores), \"Race_ID\", \"score\")\n",
    "submission = test_out[[\"Race_ID\", \"Horse\", \"Predicted_Probability\"]]\n",
    "submission.to_csv(OUTPUTS / \"test_predictions.csv\", index=False)\n",
    "submission.to_csv(OUTPUTS / \"test_preds_lgbm.csv\", index=False)\n",
    "print(\"‚úÖ test_predictions.csv and test_preds_lgbm.csv saved!\")\n",
    "\n",
    "# --- Sanity Checks\n",
    "check = submission.groupby(\"Race_ID\")[\"Predicted_Probability\"].sum()\n",
    "assert check.between(0.999, 1.001).all(), \"Probabilities do not sum to 1 per race!\"\n",
    "\n",
    "# ==========================================================\n",
    "# 2. Random Forest (sklearn)\n",
    "# ==========================================================\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import log_loss, brier_score_loss\n",
    "\n",
    "NUM_FOLDS = 5\n",
    "gkf = GroupKFold(n_splits=NUM_FOLDS)\n",
    "oof_scores_rf = np.zeros(len(X))\n",
    "models_rf = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(X, target, groups=groups)):\n",
    "    print(f\"\\nüå≤ RF Training Fold {fold + 1}/{NUM_FOLDS}...\")\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200, max_depth=8, min_samples_leaf=10,\n",
    "        random_state=42, n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X.iloc[train_idx], target.iloc[train_idx])\n",
    "    val_scores = rf.predict_proba(X.iloc[val_idx])[:, 1]\n",
    "    oof_scores_rf[val_idx] = val_scores\n",
    "    models_rf.append(rf)\n",
    "\n",
    "oof_rf = train[[\"Race_ID\", \"Horse\", \"Winner\"]].copy()\n",
    "oof_rf[\"Predicted_Probability\"] = softmax_group(oof_rf.assign(score=oof_scores_rf), \"Race_ID\", \"score\")\n",
    "save_predictions(\"oof_preds_rf.csv\", oof_rf.rename(columns={\"Winner\": \"True_Label\"}))\n",
    "\n",
    "test_rf = test[[\"Race_ID\", \"Horse\"]].copy()\n",
    "rf_test_scores = np.mean([m.predict_proba(X_test)[:, 1] for m in models_rf], axis=0)\n",
    "test_rf[\"Predicted_Probability\"] = softmax_group(test_rf.assign(score=rf_test_scores), \"Race_ID\", \"score\")\n",
    "save_predictions(\"test_preds_rf.csv\", test_rf)\n",
    "\n",
    "# ==========================================================\n",
    "# 3. XGBoost\n",
    "# ==========================================================\n",
    "import xgboost as xgb\n",
    "\n",
    "oof_scores_xgb = np.zeros(len(X))\n",
    "models_xgb = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(X, target, groups=groups)):\n",
    "    print(f\"\\nü¶æ XGB Training Fold {fold + 1}/{NUM_FOLDS}...\")\n",
    "    xgb_clf = xgb.XGBClassifier(\n",
    "        n_estimators=300, max_depth=7, learning_rate=0.08,\n",
    "        subsample=0.8, colsample_bytree=0.8, use_label_encoder=False,\n",
    "        eval_metric=\"logloss\", random_state=42, tree_method='hist', verbosity=0, n_jobs=-1\n",
    "    )\n",
    "    xgb_clf.fit(X.iloc[train_idx], target.iloc[train_idx])\n",
    "    val_scores = xgb_clf.predict_proba(X.iloc[val_idx])[:, 1]\n",
    "    oof_scores_xgb[val_idx] = val_scores\n",
    "    models_xgb.append(xgb_clf)\n",
    "\n",
    "oof_xgb = train[[\"Race_ID\", \"Horse\", \"Winner\"]].copy()\n",
    "oof_xgb[\"Predicted_Probability\"] = softmax_group(oof_xgb.assign(score=oof_scores_xgb), \"Race_ID\", \"score\")\n",
    "save_predictions(\"oof_preds_xgb.csv\", oof_xgb.rename(columns={\"Winner\": \"True_Label\"}))\n",
    "\n",
    "test_xgb = test[[\"Race_ID\", \"Horse\"]].copy()\n",
    "xgb_test_scores = np.mean([m.predict_proba(X_test)[:, 1] for m in models_xgb], axis=0)\n",
    "test_xgb[\"Predicted_Probability\"] = softmax_group(test_xgb.assign(score=xgb_test_scores), \"Race_ID\", \"score\")\n",
    "save_predictions(\"test_preds_xgb.csv\", test_xgb)\n",
    "\n",
    "# ==========================================================\n",
    "# 4. CatBoostClassifier\n",
    "# ==========================================================\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "oof_scores_cat = np.zeros(len(X))\n",
    "models_cat = []\n",
    "cat_features_idx = [X.columns.get_loc(col) for col in cat_cols if col in X.columns]\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(X, target, groups=groups)):\n",
    "    print(f\"\\nüêà CatBoost Training Fold {fold + 1}/{NUM_FOLDS}...\")\n",
    "    train_pool = Pool(X.iloc[train_idx], label=target.iloc[train_idx], cat_features=cat_features_idx)\n",
    "    val_pool = Pool(X.iloc[val_idx], label=target.iloc[val_idx], cat_features=cat_features_idx)\n",
    "    cat = CatBoostClassifier(\n",
    "        iterations=500, learning_rate=0.08, depth=7, loss_function='Logloss',\n",
    "        eval_metric='Logloss', random_seed=42, verbose=100, early_stopping_rounds=50\n",
    "    )\n",
    "    cat.fit(train_pool, eval_set=val_pool, use_best_model=True, verbose=100)\n",
    "    val_scores = cat.predict_proba(X.iloc[val_idx])[:, 1]\n",
    "    oof_scores_cat[val_idx] = val_scores\n",
    "    models_cat.append(cat)\n",
    "\n",
    "oof_cat = train[[\"Race_ID\", \"Horse\", \"Winner\"]].copy()\n",
    "oof_cat[\"Predicted_Probability\"] = softmax_group(oof_cat.assign(score=oof_scores_cat), \"Race_ID\", \"score\")\n",
    "save_predictions(\"oof_preds_cat.csv\", oof_cat.rename(columns={\"Winner\": \"True_Label\"}))\n",
    "\n",
    "test_cat = test[[\"Race_ID\", \"Horse\"]].copy()\n",
    "cat_test_scores = np.mean([m.predict_proba(X_test)[:, 1] for m in models_cat], axis=0)\n",
    "test_cat[\"Predicted_Probability\"] = softmax_group(test_cat.assign(score=cat_test_scores), \"Race_ID\", \"score\")\n",
    "save_predictions(\"test_preds_cat.csv\", test_cat)\n",
    "\n",
    "# ==========================================================\n",
    "# 5. MLP Neural Network (sklearn)\n",
    "# ==========================================================\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "oof_scores_mlp = np.zeros(len(X))\n",
    "models_mlp = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(X, target, groups=groups)):\n",
    "    print(f\"\\nüß† MLP Training Fold {fold + 1}/{NUM_FOLDS}...\")\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=200, alpha=1e-4, random_state=42)\n",
    "    mlp.fit(X.iloc[train_idx], target.iloc[train_idx])\n",
    "    val_scores = mlp.predict_proba(X.iloc[val_idx])[:, 1]\n",
    "    oof_scores_mlp[val_idx] = val_scores\n",
    "    models_mlp.append(mlp)\n",
    "\n",
    "oof_mlp = train[[\"Race_ID\", \"Horse\", \"Winner\"]].copy()\n",
    "oof_mlp[\"Predicted_Probability\"] = softmax_group(oof_mlp.assign(score=oof_scores_mlp), \"Race_ID\", \"score\")\n",
    "save_predictions(\"oof_preds_mlp.csv\", oof_mlp.rename(columns={\"Winner\": \"True_Label\"}))\n",
    "\n",
    "test_mlp = test[[\"Race_ID\", \"Horse\"]].copy()\n",
    "mlp_test_scores = np.mean([m.predict_proba(X_test)[:, 1] for m in models_mlp], axis=0)\n",
    "test_mlp[\"Predicted_Probability\"] = softmax_group(test_mlp.assign(score=mlp_test_scores), \"Race_ID\", \"score\")\n",
    "save_predictions(\"test_preds_mlp.csv\", test_mlp)\n",
    "\n",
    "print(\"\\nüéØ All model OOF/test predictions saved for stacking or analysis.\")\n",
    "\n",
    "# %%\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
